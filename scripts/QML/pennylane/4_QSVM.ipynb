{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://quantumspain-project.es/wp-content/uploads/2022/11/Logo_QS_EspanaDigital.png\" width=\"1000px\"/><br><br><br><br>\n",
    "\n",
    "\n",
    "# QML (Quantum Machine Learning)\n",
    "\n",
    "Created: 2022/10/30\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img aling=\"left\" alt=\"Licencia Creative Commons\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />License: <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Licencia Creative Commons Atribución-CompartirIgual 4.0 Internacional</a>.\n",
    "Internal Reviewers:\n",
    "* Alba Cervera ([BSC](https://www.bsc.es/))\n",
    "\n",
    "Authors:\n",
    "* Carmen Calvo ([SCAYLE](https://www.scayle.es/))\n",
    "* Antoni Alou ([PIC](https://www.pic.es/))\n",
    "* Carlos Hernani ([UV](https://www.uv.es/))\n",
    "* Nahia Iriarte ([NASERTIC](https://www.nasertic.es/es))\n",
    "* Carlos Luque ([IAC](https://www.iac.es/))\n",
    "\n",
    "$ \\newcommand{\\bra}[1]{\\langle #1|} $\n",
    "$ \\newcommand{\\ket}[1]{|#1\\rangle} $\n",
    "$ \\newcommand{\\braket}[2]{\\langle #1|#2\\rangle} $\n",
    "$ \\newcommand{\\ketbra}[2]{| #1\\rangle \\langle #2|} $\n",
    "$ \\newcommand{\\i}{{\\color{blue} i}} $ \n",
    "$ \\newcommand{\\Hil}{{\\mathbb H}} $\n",
    "$ \\newcommand{\\boldn}{{\\bf n}} $\n",
    "$ \\newcommand{\\tr}{{\\rm tr}}$\n",
    "$ \\newcommand{\\bn}{{\\bf n}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulta la notación que se ha utilizado durante todo el documento en el siguiente [enlace](#notacion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Quantum Support Vector Machines (QSVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se tratarán las **máquinas de vectores de soporte o de soporte vectorial** (en inglés *Support Vector Machines*, SVM) tanto en computación clásica como en computación cuántica. En este último caso, la solución es conocida como *máquinas de vectores de soporte cuánticos*. A su vez, se presentarán algunas implementaciones de ambas computaciones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='SVM'></a>\n",
    "## 4.1. Support Vector Machines (SVM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las máquinas de vectores de soporte o de soporte vectorial son un conjunto de modelos de aprendizaje supervisado propuesto por Vladimir Vapnik .atl en 1964 y posteriormente fueron mejorados en diferentes versiones [[1, 2]](#referencias). Los SVMs se puede aplicar en procesos de clasificación, regresión y detección de valores atípicos (en inglés, *outliers*).\n",
    "\n",
    "El SVM construye el hiperplano óptimo que maximiza la distancia entre dos categorías del conjunto de datos para una mejor predicción. Es decir, buscar el hiperplano con la distancia de separación máxima entre las dos categorías del conjunto de datos. Este hecho, lo diferencia de los modelos de perceptrón que buscan el hiperplano que distinga las dos categorías del conjunto de datos. En versiones posteriores, el SVM permite clasificar conjuntos de datos multicategoría mediante la generación de un conjunto de hiperplanos. Intuitivamente, una buena separación se consigue mediante el hiperplano que tiene la mayor distancia al punto de datos de entrenamiento más cercano de cualquier categoría (el llamado margen funcional), ya que en general cuanto mayor es el margen, menor es el error de generalización del clasificador [[7]](#referencias).\n",
    "\n",
    "<img src=\"imagenes/Svm_separating_hyperplanes.png\" alt=\"SVM distintos hiperplanos\" width=\"300\"/>\n",
    "<center>Imagen 1. SVM con distintos hiperplanos.</center>\n",
    "\n",
    "En la imagen 1 se visualizan tres posibles hiperplanos para separar las dos clases del conjunto de datos, puntos negros y blancos. Los ejes $x_{1}$ y $x_{2}$ corresponden a las características de cada instancia en el conjunto de datos, conocidos como **valores de características**, en este caso son las coordenadas de los puntos. Los valores de características de una instanacia, $\\mathbf{x_i}$, se representan en un **vector de características**.  El hiperplano H1,  no separa adecuadamente las categorías o clases del conjunto de datos; el hiperplano H2 las separa con un margen pequeño y, en cambio, el hiperplano H3 las distingue con la distancia óptima. En consecuencia, se genera la frontera de decisión adecuada u óptima para la predicción. En otras palabras, el SVM con el hiperplano H3 distinguirá con una mayor probabilidad la clase a la que pertenece un nuevo punto, este proceso se conoce como **clasificación**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><p style=\"text-align:center\"> >> Explicación formal >> </p></summary>\n",
    "\n",
    "A continuación, se explicará formalmente el proceso de clasificación lineal que puede ser extrapolado a la clasificación no lineal a través del kernel.\n",
    "\n",
    "La idea del algoritmo SVC es maximizar el margen entre los ejemplos pertenecientes a clases opuestas. En primer lugar, se deben conocer los ejemplos que definen dicho margen, en otras palabras, hay que identificar la instancia de la clase positiva más cercana a las instancias de la clase negativa y viceversa. Estos ejemplos se conocen como **vectores de soporte** y definirán dos planos paralelos al hiperplano óptimo como se muestra en la imagen 2.\n",
    "\n",
    "<img src=\"imagenes/SVM_margin.png\" width=\"300\"/>\n",
    "<center>Imagen 2. Hiperplano de margen máximo y márgenes para una SVM entrenada con muestras de dos clases. Las muestras del margen se denominan vectores de soporte [6] </center>\n",
    "\n",
    "\n",
    "El hiperplano se define con la fórmula $\\mathbf{wx} + b = 0$, donde  $\\mathbf{x}$ corresponde al vector de $N$ características de cada instancia en el conjunto de entrenamiento, $ \\mathbf{x} \\in \\Re^{N}$, $\\mathbf{w}$ corresponde al vector normal del hiperplano, $ \\mathbf{w} \\in \\Re^{N}$ y $b$ es una constante, $b \\in \\Re$, conocida *bias*. Estos últimos son los hiperparámetros que el algoritmo de SVC intenta definir en el proceso de entrenamiento y, además, proporcionan la distancia del hiperplano al origen de coordenadas mediante la expresión $\\frac{b}{||\\mathbf{w}||}$. \n",
    "\n",
    "El hiperplano óptimo deberá satisfacer: \n",
    "- $\\mathbf{wx} + b > 1$ para una clase\n",
    "- $\\mathbf{wx} + b < -1$ para la otra clase\n",
    "- $\\mathbf{wx}  + b \\geq 1$ para los vectores de soporte.\n",
    "\n",
    "Los planos definidos por los vectores soportes se expresan en las ecuaciones:\n",
    "- $\\mathbf{wx_{+}}  +b = 1$ \n",
    "- $\\mathbf{wx_{-}}  + b = -1$\n",
    "\n",
    "\n",
    "Con los planos definidos por los vectores de soporte, se calcula la longitud del margen generado entre los planos donde están ubicados los vectores de soporte, $\\mathbf{x_-}, \\mathbf{x_+}$. Para ello es necesario conocer la distancia de cada vector de soporte al origen, de aquí en adelante se hará referencia a ellas con $d_+$ para la distancia entre el vector de soporte de la clase positiva y el origen y $d_-$ en el caso contrario. Estas distancias se obtienen como sigue:\n",
    "\n",
    "$$d_- = \\frac{\\mathbf{x_-}\\cdot \\mathbf{w}}{||\\mathbf{w}||}, ~~~~~~~ d_+ = \\frac{\\mathbf{x_+}\\cdot \\mathbf{w}}{||\\mathbf{w}||} $$ \n",
    "\n",
    "Una vez conocidas estas distancias se conoce el margen entre las clases:\n",
    "\n",
    "$$ d = d_+-d_- = \\frac{1}{||\\mathbf{w}||} (\\mathbf{x_+} \\cdot \\mathbf{w} - \\mathbf{x_-}\\cdot \\mathbf{w})$$\n",
    "\n",
    "Cabe destacar que para los vectores de soporte se cumple $$y_i(\\mathbf{w} \\cdot \\mathbf{x} + b) - 1 = 0$$ por lo que $$d = \\frac{1}{||\\mathbf{w}||} (1-b+1+b) = \\frac{2}{||\\mathbf{w}||}$$\n",
    "\n",
    "Tal y como se comentaba anteriormente, el objetivo del SVM es maximizar $\\frac{2}{||\\mathbf{w}||}$ o lo que es equivalente, minimizar $\\frac{1}{2}||\\mathbf{w}||^2$ siempre teniendo en cuenta la restricción que imponen los vectores de soporte, $y_i(\\mathbf{w} \\cdot \\mathbf{x_{\\pm}} + b) - 1 = 0$.\n",
    "\n",
    "La resolución de este problema de optimización se lleva a cabo mediante operadores de Lagrange. A partir de la función objetivo $f(\\mathbf{w}) = \\frac{1}{2}||\\mathbf{w}||^2 = \\frac{1}{2}\\mathbf{w}^T \\mathbf{w}$  y la restricción $y_i(\\mathbf{w} \\cdot \\mathbf{x_{\\pm}} + b) - 1 = 0$ se define un nuevo problema de optimización con la siguiente función objetivo: $$ L(\\mathbf{w}, b, \\alpha_i) = \\frac{1}{2}\\mathbf{w}^T \\mathbf{w} - \\sum _{i=1}^l \\alpha_i[y_i(\\mathbf{w}\\cdot \\mathbf{x_i} + b)-1]$$ donde $l$ es el número de vectores de soporte.\n",
    "\n",
    "Para obtener el mínimo de la función objetivo, se calcula la derivada respecto a $\\mathbf{w}$ y $b$ y se iguala a cero. Tras realizar este cálculo se obtiene: $$ \\frac{\\partial L}{\\partial \\mathbf{w}} = 0 \\Longrightarrow \\mathbf{w}  = \\sum_{i=1}^l \\alpha_iy_i\\mathbf{x_i} ~~~~~~~~~~~~~~~~ \\frac{\\partial L}{\\partial b} = 0 \\Longrightarrow \\sum_{i=1}^l \\alpha_iy_i = 0$$\n",
    "\n",
    "Reemplazando estas dos expresiones en la función objetivo se obtiene la siguiente expresión \n",
    "\n",
    "$$L(\\alpha_i)= \\sum_i^l \\alpha_i - \\frac{1}{2} \\sum_{i,j}^l \\alpha_i\\alpha_j y_iy_j \\mathbf{x_{i}x_{j}}$$\n",
    "\n",
    "La función objetivo o función de coste, $L(\\alpha_i)$ es minimizada vía optimización de los parámetros $\\vec{\\alpha}$. \n",
    "\n",
    "Una vez resuelto el problema de optimización y el entrenamiento, la **función de decisión** corresponde a: $$ y_i = \\sum_{i}^l  y_i \\alpha_i \\mathbf{x_ {i}x} + b $$\n",
    "\n",
    "Por tanto, la clasificación de un vector de características \"$\\mathbf{x}$\", conocer su clase, será realizada con la siguiente fórmula $$y_{i, \\mathbf{x}} = \\text{sign}(\\sum_{i}^l  y_i \\alpha_i \\mathbf{x_{i}x} + b)$$\n",
    "\n",
    "Este fórmula no funciona correctamente si el conjunto de datos no es linealmente separabls. Estos son los casos en los que pueden resultar útiles los métodos kernel, ya que tratan de mapear los datos en otro espacio de características con mayor dimensión. En resumen, la función desición con kernel correspenderá a: $$y_{i, \\mathbf{x}} = \\text{sign}(\\sum_{i}^l  y_i \\alpha_i K(\\mathbf{x_i,x}) + b)$$\n",
    "\n",
    "Para más detalle, puedes visualizar [[9]](#referencias)\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='QSVC'></a>\n",
    "## 4.2. Clasificador de vectores de soporte cuántico (QSVC, en sus siglas en inglés)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El QSVC es una combinación de computación clásico con la computación cuántica como se puede observar en la imagen 3. Esta técnica aprovecha la capacidad del SVC de trabajar con kernels que pueden ser calculados en computación cuántica y beneficiándose de la alta dimensionalidad en el espacio de Hilbert  [[8, 5]](#references). Para más detalles de los métodos kernel, visualice el notebook [*5_Kernel*](5_Kernel.ipynb).\n",
    "\n",
    "\n",
    "<img src=\"imagenes/qsvc.svg\" alt=\"qsvc\" width=\"500\"/>\n",
    "<center>Imagen 3. Flujo de trabajo del QSVM [3].</center>\n",
    "\n",
    "\n",
    "El QSVC corresponde en varios pasos como se visualiza en la imagen 3:\n",
    "1. La generación de kernel cuántico, $K(\\mathbf{x_{i}}, \\mathbf{x_{j}}) = |\\bra{0^N} \\mathcal{U}^\\dagger_{ \\Phi(\\mathbf{x_{i}})} \\mathcal{U}_{ \\Phi(\\mathbf{x_{j}})} \\ket{0^N} |^2$ donde $N$ es el número de características o qubits y se interpreta como una medida de similitud entre *qubits*:\n",
    "    1. Mapea cada dato clásico, $\\mathbf{x_{i}}$, de un conjunto a datos cuánticos $\\vert \\Phi{(\\mathbf{x_{i}})} \\rangle$. Esto se puede lograr por el circuito cuántico $\\mathcal{U}_{\\Phi{(\\mathbf{x_{i}})}} \\vert 0^N \\rangle $ donde el valor inicial de cada qubit es $\\ket{0}$. \n",
    "    2. Obtención de la matriz del kernel, cada elemento de la matriz del kernel se calcula,  $K_{i,j} = |\\braket{\\Phi(\\mathbf{x_{i}})^\\dagger,\\Phi(\\mathbf{x_{j}})}|^2 = |\\bra{0^N} \\mathcal{U}^\\dagger_{ \\Phi(\\mathbf{x_{i}})} \\mathcal{U}_{ \\Phi{(\\mathbf{x_{j}})}} \\ket{0^N}|^2 $ donde $i,j = 0...M$ y $M$ es el número de vectores de características en el conjunto de datos. \n",
    "2. Se aplica la matriz de kernel en el algoritmo de SVC clásico para entrenar y obtener la predicción. \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='EjemploQSVC'></a>\n",
    "### 4.2.1. Ejemplo de QSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ejemplo se basa en un tutorial de pennyline [[4]](#referencias). En este, se usará el SVM de la librería *scikit-learn*, *sklearn*. La interfaz permite proporcionar kernel adicional en dos formas:\n",
    "1. La función que calcula la matriz del kernel. \n",
    "    - (kernel= nombrefunción)\n",
    "2. La matriz del kernel calculada. \n",
    "    - (kernel=\"precomputed\")  \n",
    "\n",
    "En el ejemplo, se aplicará la primera forma, facilitando la función que calcula la matriz del kernel. A su vez, el famoso conjunto de datos, *Iris* [[10]](#referencias). Este conjunto de datos, que data de 1936, consta de 150 muestras de cuatro características cada una y da lugar a un problema de clasificación y se utilizará solo las primeras 100 muestras. \n",
    "\n",
    "\n",
    "Se cargan las librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga el conjunto de datos, $X$, las primeras 100 muestras y se adaptan los datos aplicando un escalado periódico por la codificación a datos cuánticos, es decir, la *distribución normal estándar*,también conocida como *la distribución Z*.  \n",
    "\n",
    "A su vez, el conjunto de datos se divide en dos grupos: \n",
    "* Entrenamiento\n",
    "* Testeo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Se selecciona solo la dos primeras clases del conjunto \n",
    "# de datos debido a que el clasificador es binario\n",
    "X = X[:100]\n",
    "y = y[:100]\n",
    "\n",
    "# Escalar los datos a distribución normal estándar debido a la \n",
    "# codificación datos es periódica\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Escalar las etiquetas a -1 y 1 debido que es importante para SVM\n",
    "y_scaled = 2 * (y - 0.5)\n",
    "\n",
    "# Se divide en entrenamiento y testeo\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La codificación de los datos clásicos a datos cuánticos se llevará a cabo mediante la codificación en ángulo, función *AngleEmbedding* en pennylane. Para más detalles de codificación cuántica, visualice el notebook [*2_Feature enconding*](2_Feature_encoding.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codificacionCuantica(x, nqbits):\n",
    "    qml.AngleEmbedding(x, wires=range(nqbits))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como cada vector de características se constituye de cuatro características, se necesitarán cuatro qubits para codificar a datos cuánticos y por tanto $N = 4$.\n",
    "\n",
    "Se define el kernel cuántico con el siguiente circuito cuántico, se usa el **evaluador de kernel cuántico**, en inglés *Quantum Kernel Evaluator, QKE*. Para más detalles de los kernels, visualice el notebook [*5_Kernel*](5_Kernel.ipynb).\n",
    "\n",
    "En el ejemplo, el circuito cuántico representa la función del kernel, $K(\\mathbf{x_{i}}, \\mathbf{x_{j}})$ = $|U(\\mathbf{x_{i}})^{\\dagger}U(\\mathbf{x_{j}})\\ket{0^N}|^2$ y un valor de la kernel, $K_{ij}$, se calcula como la probabilidad de $|\\langle {0^N}|\\phi \\rangle|^2$ de medir la base computacional $\\ket{0^N}$ del estado cuántico actual $\\ket{\\phi} $ (en pennylane, se obtiene con la función *probs*). En otras palabras, es la estimación de observar el estado inicial $\\ket{\\phi} = \\ket{0^N} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuatro cúbits: cuatro características\n",
    "n_qubits = len(X_train[0]) \n",
    "\n",
    "dev_kernel = qml.device(\"default.qubit\", wires = n_qubits)      \n",
    "@qml.qnode(dev_kernel)\n",
    "\n",
    "# kernel\n",
    "def kernel_circuit(x,y):\n",
    "    _qbits = len(dev_kernel.wires)\n",
    "    codificacionCuantica(y, _qbits)\n",
    "    qml.adjoint(codificacionCuantica)(x, _qbits)\n",
    "    probs = qml.probs(wires = dev_kernel.wires)\n",
    "    return probs\n",
    "\n",
    "kernel = lambda x1, x2: kernel_circuit(x1, x2)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifica que el kernel cuántico funciona correctamente con el vector de característica, $\\mathbf{x}$, consigo mismo. El resultado debería ser 1, es decir, la probalididad del estado inicial, $\\ket{\\phi}_{inicial} = \\ket{0^N} $,  es el estado final $\\ket{\\phi}_{final} = \\ket{0^N} $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "kernel(x,x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El circuito cuántico para el vector de característica, $\\mathbf{x}$, es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ──RX(1.00)──RX(1.00)†─┤ ╭Probs\n",
      "1: ──RX(2.00)──RX(2.00)†─┤ ├Probs\n",
      "2: ──RX(3.00)──RX(3.00)†─┤ ├Probs\n",
      "3: ──RX(4.00)──RX(4.00)†─┤ ╰Probs\n"
     ]
    }
   ],
   "source": [
    "print(qml.draw(kernel_circuit, expansion_strategy=\"device\")(x,x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define una función que calcula una matriz de kernel para los vectores de características en dos conjuntos de datos diferentes $A, B$. Si $A=B$, la matriz de kernel es conocida como la **matriz Gram** [[11]](#referencias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_matrix(A, B):\n",
    "    return np.array([[kernel(a, b) for b in B] for a in A])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el entrenamiento, se inicializa el clasificador SVM con la matriz del kernel cuántico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando...\n"
     ]
    }
   ],
   "source": [
    "# Inicialización\n",
    "qsvm = svm.SVC(kernel=kernel_matrix)\n",
    "\n",
    "print(\"Entrenando...\")\n",
    "qsvm = qsvm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se calcula la precisión en la clasificación con el conjunto de datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión con test...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "print(\"Precisión con test...\")\n",
    "\n",
    "predictions = qsvm.predict(X_test)\n",
    "accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se compara el resultado con un SVM clásico con kernel lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión con test...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelclasico = svm.SVC(kernel=\"linear\")\n",
    "modelclasico.fit(X_train, y_train)\n",
    "\n",
    "print(\"Precisión con test...\")\n",
    "\n",
    "predictions = modelclasico.predict(X_test)\n",
    "accuracy_score(predictions, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='notacion'></a>\n",
    "<div class=\"alert alert-block alert-warning\",text-align:center>\n",
    "<b>ANEXO NOTACIÓN:</b>\n",
    "\n",
    "Para que la comprensión de los notebooks sea mejor se ha unificado la notación utilizada en los mismos. Para diferenciar un vector de un valor único se hará uso de la negrita. De manera que $\\mathbf{x}$ corresponde a un vector y $z$ será una variable de una única componente. \n",
    "\n",
    "    \n",
    "Si se quiere hacer referencia a dos vectores distintos pero que pertenecen al mismo *dataset* se utilizará un subíndice, es decir, $\\mathbf{x_i}$ hará referencia al i-ésimo vector del dataset. Si se quiere referenciar una característica concreta del vector $\\mathbf{x_i}$ se añadirá un nuevo subíndice, de manera que $\\mathbf{x_{i_j}}$ hará referencia a la j-ésima variable del i-ésimo vector.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "## Referencias\n",
    "<a id='referencias'></a>\n",
    "\n",
    "[1] https://scikit-learn.org/stable/modules/svm.html#\n",
    "\n",
    "[2] Cortes, Corinna; Vapnik, Vladimir (1995). \"Support-vector networks\" (PDF). Machine Learning. 20 (3): 273–297.\n",
    "\n",
    "[3] https://learn.qiskit.org/course/machine-learning/quantum-feature-maps-kernels\n",
    "\n",
    "[4] https://pennylane.ai/qml/demos/tutorial_kernel_based_training.html\n",
    "\n",
    "[5] Havlíček et al. Supervised learning with quantum-enhanced feature spaces. Nature 567, 209 (2019)\n",
    "\n",
    "[6] https://en.wikipedia.org/wiki/Support_vector_machine#cite_note-4\n",
    "\n",
    "[7] Hastie, Trevor; Tibshirani, Robert; Friedman, Jerome (2008). The Elements of Statistical Learning : Data Mining, Inference, and Prediction (Second ed.). New York: Springer. p. 134.\n",
    "\n",
    "[8] Maria Schuld and Nathan Killoran, Quantum Machine Learning in Feature Hilbert Spaces. https://doi.org/10.1103%2Fphysrevlett.122.040504\n",
    "\n",
    "[9] https://github.com/epfml/ML_course/blob/master/lectures/07/lecture07a_svm.pdf\n",
    "\n",
    "[10] R. A. Fisher (1936). «The use of multiple measurements in taxonomic problems» https://web.archive.org/web/20110928044802/http://digital.library.adelaide.edu.au/coll/special//fisher/138.pdf\n",
    "\n",
    "[11] Barth, Nils (1999). «The Gramian and K-Volume in N-Space: Some Classical Results in Linear Algebra». Journal of Young Investigators 2. Archivado desde el original el 22 de noviembre de 2008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work has been financially supported by the Ministry of Economic Affairs and Digital Transformation of the Spanish Government through the QUANTUM ENIA project call - Quantum Spain project, and by the European Union through the Recovery, Transformation and Resilience Plan - NextGenerationEU within the framework of the Digital Spain 2025 Agenda.\n",
    "\n",
    "\n",
    "<img align=\"left\" src=\"https://quantumspain-project.es/wp-content/uploads/2022/11/LOGOS-GOB_QS.png\" width=\"1000px\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8edca44e8d84eb6663382c0430fc4eb903083b0f428b9d0b350f19027e85790"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
